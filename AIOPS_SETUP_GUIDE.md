# Backend AIops Setup Guide

ì´ ë¬¸ì„œëŠ” `backend_aiops` ì €ì¥ì†Œë¥¼ ìƒˆë¡œ ìƒì„±í•˜ê³  ì„¤ì •í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## 1. ì €ì¥ì†Œ êµ¬ì¡°

```
backend_aiops/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ cd.yml
â”œâ”€â”€ aiops/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ probability_calculate/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ coastal_flood_probability_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cold_wave_probability_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ drought_probability_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ high_temperature_probability_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ inland_flood_probability_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ typhoon_probability_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ urban_flood_probability_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ water_scarcity_probability_agent.py
â”‚   â”‚   â”‚   â””â”€â”€ wildfire_probability_agent.py
â”‚   â”‚   â””â”€â”€ hazard_calculate/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ coastal_flood_hscore_agent.py
â”‚   â”‚       â”œâ”€â”€ cold_wave_hscore_agent.py
â”‚   â”‚       â”œâ”€â”€ drought_hscore_agent.py
â”‚   â”‚       â”œâ”€â”€ high_temperature_hscore_agent.py
â”‚   â”‚       â”œâ”€â”€ inland_flood_hscore_agent.py
â”‚   â”‚       â”œâ”€â”€ typhoon_hscore_agent.py
â”‚   â”‚       â”œâ”€â”€ urban_flood_hscore_agent.py
â”‚   â”‚       â”œâ”€â”€ water_scarcity_hscore_agent.py
â”‚   â”‚       â””â”€â”€ wildfire_hscore_agent.py
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ probability_batch.py
â”‚   â”‚   â”œâ”€â”€ hazard_batch.py
â”‚   â”‚   â”œâ”€â”€ probability_scheduler.py
â”‚   â”‚   â””â”€â”€ hazard_scheduler.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ connection.py
â”‚   â””â”€â”€ triggers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ notify_listener.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ main.py
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

## 2. í•„ìˆ˜ íŒŒì¼ ìƒì„±

### 2.1 pyproject.toml

```toml
[project]
name = "backend-aiops"
version = "0.1.0"
description = "AIops workflow for climate risk batch processing"
requires-python = ">=3.11"
dependencies = [
    "apscheduler>=3.10.4",
    "psycopg2-binary>=2.9.9",
    "python-dotenv>=1.0.0",
    "pydantic>=2.5.0",
    "pydantic-settings>=2.1.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.uv]
dev-dependencies = [
    "pytest>=7.4.3",
    "pytest-cov>=4.1.0",
]
```

### 2.2 .env.example

```env
# Database Configuration
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=climate_risk_db
DATABASE_USER=postgres
DATABASE_PASSWORD=your_password

# Scheduler Configuration
PROBABILITY_SCHEDULE_MONTH=1
PROBABILITY_SCHEDULE_DAY=1
PROBABILITY_SCHEDULE_HOUR=2
PROBABILITY_SCHEDULE_MINUTE=0

HAZARD_SCHEDULE_MONTH=1
HAZARD_SCHEDULE_DAY=1
HAZARD_SCHEDULE_HOUR=4
HAZARD_SCHEDULE_MINUTE=0

# Batch Processing Configuration
PARALLEL_WORKERS=4
BATCH_SIZE=1000

# PostgreSQL LISTEN/NOTIFY
NOTIFY_CHANNEL=aiops_trigger
```

### 2.3 aiops/config/settings.py

```python
from pydantic_settings import BaseSettings
from typing import Optional


class Settings(BaseSettings):
    # Database
    database_host: str = "localhost"
    database_port: int = 5432
    database_name: str = "climate_risk_db"
    database_user: str = "postgres"
    database_password: str = ""

    # Scheduler
    probability_schedule_month: int = 1
    probability_schedule_day: int = 1
    probability_schedule_hour: int = 2
    probability_schedule_minute: int = 0

    hazard_schedule_month: int = 1
    hazard_schedule_day: int = 1
    hazard_schedule_hour: int = 4
    hazard_schedule_minute: int = 0

    # Batch Processing
    parallel_workers: int = 4
    batch_size: int = 1000

    # NOTIFY
    notify_channel: str = "aiops_trigger"

    class Config:
        env_file = ".env"
        case_sensitive = False


settings = Settings()
```

### 2.4 aiops/database/connection.py

```python
import psycopg2
from psycopg2.extras import RealDictCursor
from contextlib import contextmanager
from typing import List, Dict, Any
from ..config.settings import settings


class DatabaseConnection:
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬"""

    @staticmethod
    def get_connection_string() -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ìì—´ ìƒì„±"""
        return (
            f"host={settings.database_host} "
            f"port={settings.database_port} "
            f"dbname={settings.database_name} "
            f"user={settings.database_user} "
            f"password={settings.database_password}"
        )

    @staticmethod
    @contextmanager
    def get_connection():
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        conn = psycopg2.connect(
            DatabaseConnection.get_connection_string(),
            cursor_factory=RealDictCursor
        )
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()

    @staticmethod
    def fetch_grid_coordinates() -> List[Dict[str, float]]:
        """ëª¨ë“  ê²©ì ì¢Œí‘œ ì¡°íšŒ"""
        with DatabaseConnection.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT DISTINCT latitude, longitude
                FROM climate_data
                ORDER BY latitude, longitude
            """)
            return [dict(row) for row in cursor.fetchall()]

    @staticmethod
    def fetch_climate_data(latitude: float, longitude: float) -> Dict[str, Any]:
        """íŠ¹ì • ê²©ìì˜ ê¸°í›„ ë°ì´í„° ì¡°íšŒ"""
        with DatabaseConnection.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT * FROM climate_data
                WHERE latitude = %s AND longitude = %s
                ORDER BY year, month
            """, (latitude, longitude))
            return [dict(row) for row in cursor.fetchall()]

    @staticmethod
    def save_probability_results(results: List[Dict[str, Any]]) -> None:
        """P(H) ê³„ì‚° ê²°ê³¼ ì €ì¥"""
        with DatabaseConnection.get_connection() as conn:
            cursor = conn.cursor()
            for result in results:
                cursor.execute("""
                    INSERT INTO probability_results
                    (latitude, longitude, risk_type, probability, bin_data, calculated_at)
                    VALUES (%(latitude)s, %(longitude)s, %(risk_type)s,
                            %(probability)s, %(bin_data)s, NOW())
                    ON CONFLICT (latitude, longitude, risk_type)
                    DO UPDATE SET
                        probability = EXCLUDED.probability,
                        bin_data = EXCLUDED.bin_data,
                        calculated_at = EXCLUDED.calculated_at
                """, result)

    @staticmethod
    def save_hazard_results(results: List[Dict[str, Any]]) -> None:
        """Hazard Score ê³„ì‚° ê²°ê³¼ ì €ì¥"""
        with DatabaseConnection.get_connection() as conn:
            cursor = conn.cursor()
            for result in results:
                cursor.execute("""
                    INSERT INTO hazard_results
                    (latitude, longitude, risk_type, hazard_score,
                     hazard_score_100, hazard_level, calculated_at)
                    VALUES (%(latitude)s, %(longitude)s, %(risk_type)s,
                            %(hazard_score)s, %(hazard_score_100)s,
                            %(hazard_level)s, NOW())
                    ON CONFLICT (latitude, longitude, risk_type)
                    DO UPDATE SET
                        hazard_score = EXCLUDED.hazard_score,
                        hazard_score_100 = EXCLUDED.hazard_score_100,
                        hazard_level = EXCLUDED.hazard_level,
                        calculated_at = EXCLUDED.calculated_at
                """, result)
```

### 2.5 aiops/triggers/notify_listener.py

```python
import select
import psycopg2
import logging
from typing import Callable, Dict, Any
from ..config.settings import settings
from ..database.connection import DatabaseConnection

logger = logging.getLogger(__name__)


class NotifyListener:
    """PostgreSQL LISTEN/NOTIFYë¥¼ ì‚¬ìš©í•œ ì™¸ë¶€ íŠ¸ë¦¬ê±° ë¦¬ìŠ¤ë„ˆ"""

    def __init__(self):
        self.conn = None
        self.handlers: Dict[str, Callable] = {}

    def register_handler(self, job_type: str, handler: Callable) -> None:
        """ì‘ì—… íƒ€ì…ë³„ í•¸ë“¤ëŸ¬ ë“±ë¡

        Args:
            job_type: 'probability' ë˜ëŠ” 'hazard'
            handler: ì‹¤í–‰í•  í•¸ë“¤ëŸ¬ í•¨ìˆ˜
        """
        self.handlers[job_type] = handler
        logger.info(f"Handler registered for job type: {job_type}")

    def start_listening(self) -> None:
        """NOTIFY ë¦¬ìŠ¤ë‹ ì‹œì‘"""
        connection_string = DatabaseConnection.get_connection_string()
        self.conn = psycopg2.connect(connection_string)
        self.conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)

        cursor = self.conn.cursor()
        cursor.execute(f"LISTEN {settings.notify_channel};")
        logger.info(f"Listening on channel: {settings.notify_channel}")

        print(f"ğŸ§ Listening for PostgreSQL NOTIFY on channel '{settings.notify_channel}'...")

        while True:
            if select.select([self.conn], [], [], 5) == ([], [], []):
                continue
            else:
                self.conn.poll()
                while self.conn.notifies:
                    notify = self.conn.notifies.pop(0)
                    self._handle_notify(notify.payload)

    def _handle_notify(self, payload: str) -> None:
        """NOTIFY ë©”ì‹œì§€ ì²˜ë¦¬

        Payload í˜•ì‹: 'probability' ë˜ëŠ” 'hazard'
        """
        logger.info(f"Received NOTIFY: {payload}")

        if payload in self.handlers:
            try:
                logger.info(f"Executing handler for: {payload}")
                self.handlers[payload]()
                logger.info(f"Handler completed for: {payload}")
            except Exception as e:
                logger.error(f"Error executing handler for {payload}: {e}")
        else:
            logger.warning(f"No handler registered for job type: {payload}")

    def stop_listening(self) -> None:
        """ë¦¬ìŠ¤ë‹ ì¤‘ì§€"""
        if self.conn:
            self.conn.close()
            logger.info("Stopped listening")
```

### 2.6 aiops/batch/probability_batch.py

`backend_team/ai_agent/aiops_workflow/batch/probability_batch.py` íŒŒì¼ì„ ë³µì‚¬í•˜ë˜, ë‹¤ìŒ ìˆ˜ì •ì‚¬í•­ ì ìš©:

```python
# ì„í¬íŠ¸ ìˆ˜ì •
from ..agents.probability_calculate import (
    CoastalFloodProbabilityAgent,
    ColdWaveProbabilityAgent,
    # ... ë‚˜ë¨¸ì§€ agents
)
from ..database.connection import DatabaseConnection

class ProbabilityBatchProcessor:
    # ... ê¸°ì¡´ ì½”ë“œ ìœ ì§€

    def _fetch_climate_data(self, coordinate: Dict[str, float]) -> Dict[str, Any]:
        """ê¸°í›„ ë°ì´í„° ì¡°íšŒ (ì‹¤ì œ êµ¬í˜„)"""
        return DatabaseConnection.fetch_climate_data(
            coordinate['latitude'],
            coordinate['longitude']
        )

    def _save_results(self, coordinate: Dict[str, float],
                     probabilities: Dict[str, Any]) -> None:
        """ê²°ê³¼ ì €ì¥ (ì‹¤ì œ êµ¬í˜„)"""
        results = []
        for risk_type, data in probabilities.items():
            results.append({
                'latitude': coordinate['latitude'],
                'longitude': coordinate['longitude'],
                'risk_type': risk_type,
                'probability': data.get('probability'),
                'bin_data': data.get('bin_data')
            })
        DatabaseConnection.save_probability_results(results)
```

### 2.7 aiops/batch/hazard_batch.py

`backend_team/ai_agent/aiops_workflow/batch/hazard_batch.py` íŒŒì¼ì„ ë³µì‚¬í•˜ë˜, ë™ì¼í•œ ìˆ˜ì •ì‚¬í•­ ì ìš©:

```python
from ..agents.hazard_calculate import (
    CoastalFloodHScoreAgent,
    # ... ë‚˜ë¨¸ì§€ agents
)
from ..database.connection import DatabaseConnection

class HazardBatchProcessor:
    # ... ê¸°ì¡´ ì½”ë“œ ìœ ì§€

    def _save_results(self, coordinate: Dict[str, float],
                     hazard_scores: Dict[str, Any]) -> None:
        """ê²°ê³¼ ì €ì¥ (ì‹¤ì œ êµ¬í˜„)"""
        results = []
        for risk_type, data in hazard_scores.items():
            results.append({
                'latitude': coordinate['latitude'],
                'longitude': coordinate['longitude'],
                'risk_type': risk_type,
                'hazard_score': data.get('hazard_score'),
                'hazard_score_100': data.get('hazard_score_100'),
                'hazard_level': data.get('hazard_level')
            })
        DatabaseConnection.save_hazard_results(results)
```

### 2.8 aiops/batch/probability_scheduler.py

`backend_team/ai_agent/aiops_workflow/batch/probability_scheduler.py` íŒŒì¼ì„ ë³µì‚¬í•˜ë˜, config ì„í¬íŠ¸ ìˆ˜ì •:

```python
from ..config.settings import settings

class ProbabilityScheduler:
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.processor = ProbabilityBatchProcessor({
            'parallel_workers': settings.parallel_workers
        })

    def start(self, grid_coordinates_callback=None):
        trigger = CronTrigger(
            month=settings.probability_schedule_month,
            day=settings.probability_schedule_day,
            hour=settings.probability_schedule_hour,
            minute=settings.probability_schedule_minute
        )
        # ... ë‚˜ë¨¸ì§€ ì½”ë“œ ë™ì¼
```

### 2.9 aiops/batch/hazard_scheduler.py

ë™ì¼í•˜ê²Œ ìˆ˜ì •

### 2.10 main.py

```python
import logging
import signal
import sys
from aiops.batch.probability_scheduler import ProbabilityScheduler
from aiops.batch.hazard_scheduler import HazardScheduler
from aiops.batch.probability_batch import ProbabilityBatchProcessor
from aiops.batch.hazard_batch import HazardBatchProcessor
from aiops.triggers.notify_listener import NotifyListener
from aiops.database.connection import DatabaseConnection
from aiops.config.settings import settings

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_probability_batch():
    """P(H) ë°°ì¹˜ ì‘ì—… ì‹¤í–‰"""
    logger.info("Starting Probability batch job (triggered)")
    processor = ProbabilityBatchProcessor({
        'parallel_workers': settings.parallel_workers
    })
    grid_coordinates = DatabaseConnection.fetch_grid_coordinates()
    result = processor.process_all_grids(grid_coordinates)
    logger.info(f"Probability batch completed: {result}")


def run_hazard_batch():
    """Hazard Score ë°°ì¹˜ ì‘ì—… ì‹¤í–‰"""
    logger.info("Starting Hazard Score batch job (triggered)")
    processor = HazardBatchProcessor({
        'parallel_workers': settings.parallel_workers
    })
    grid_coordinates = DatabaseConnection.fetch_grid_coordinates()
    result = processor.process_all_grids(grid_coordinates)
    logger.info(f"Hazard batch completed: {result}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("Starting AIops workflow system")

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    prob_scheduler = ProbabilityScheduler()
    hazard_scheduler = HazardScheduler()

    prob_scheduler.start(grid_coordinates_callback=DatabaseConnection.fetch_grid_coordinates)
    hazard_scheduler.start(grid_coordinates_callback=DatabaseConnection.fetch_grid_coordinates)

    logger.info("Schedulers started")
    logger.info(f"  - Probability: {settings.probability_schedule_month}/{settings.probability_schedule_day} {settings.probability_schedule_hour}:{settings.probability_schedule_minute:02d}")
    logger.info(f"  - Hazard: {settings.hazard_schedule_month}/{settings.hazard_schedule_day} {settings.hazard_schedule_hour}:{settings.hazard_schedule_minute:02d}")

    # NOTIFY ë¦¬ìŠ¤ë„ˆ ì„¤ì •
    listener = NotifyListener()
    listener.register_handler('probability', run_probability_batch)
    listener.register_handler('hazard', run_hazard_batch)

    # Graceful shutdown ì„¤ì •
    def signal_handler(sig, frame):
        logger.info("Shutting down gracefully...")
        prob_scheduler.stop()
        hazard_scheduler.stop()
        listener.stop_listening()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # NOTIFY ë¦¬ìŠ¤ë‹ ì‹œì‘ (blocking)
    try:
        listener.start_listening()
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt")
        signal_handler(None, None)


if __name__ == "__main__":
    main()
```

### 2.11 Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install uv
RUN pip install uv

# Copy dependency files
COPY pyproject.toml .
COPY .env .env

# Install dependencies
RUN uv pip install --system -e .

# Copy application code
COPY aiops/ ./aiops/
COPY main.py .

# Run the application
CMD ["python", "main.py"]
```

### 2.12 .github/workflows/ci.yml

```yaml
name: CI

on:
  push:
    branches: [ develop, main ]
  pull_request:
    branches: [ develop, main ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install uv
      run: pip install uv

    - name: Install dependencies
      run: uv pip install --system -e ".[dev]"

    - name: Run tests
      run: pytest --cov=aiops --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### 2.13 .github/workflows/cd.yml

```yaml
name: CD - Deploy AIops

on:
  push:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository_owner }}/backend_team/aiops

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=sha,prefix={{branch}}-
          type=semver,pattern={{version}}

    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest

    steps:
    - name: Deploy to server
      uses: appleboy/ssh-action@master
      with:
        host: ${{ secrets.SERVER_HOST }}
        username: ${{ secrets.SERVER_USER }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        script: |
          cd /opt/backend_aiops
          docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:main
          docker-compose down
          docker-compose up -d
```

### 2.14 README.md

```markdown
# Backend AIops

Climate Risk AIops Workflow - Probability ë° Hazard Score ë°°ì¹˜ ê³„ì‚° ì‹œìŠ¤í…œ

## ê°œìš”

ì´ ì‹œìŠ¤í…œì€ ê¸°í›„ ìœ„í—˜ ë¶„ì„ì„ ìœ„í•œ AI Operations ì›Œí¬í”Œë¡œìš°ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- **P(H) ê³„ì‚°**: 9ëŒ€ ê¸°í›„ ë¦¬ìŠ¤í¬ë³„ í™•ë¥  ë° Binë³„ ê¸°ë³¸ ì†ìƒë¥  ê³„ì‚°
- **Hazard Score ê³„ì‚°**: 9ëŒ€ ê¸°í›„ ë¦¬ìŠ¤í¬ë³„ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
- **ìŠ¤ì¼€ì¤„ë§**: ì—° 1íšŒ ìë™ ì‹¤í–‰ (1ì›” 1ì¼)
- **ìˆ˜ë™ íŠ¸ë¦¬ê±°**: PostgreSQL NOTIFYë¥¼ í†µí•œ ì¦‰ì‹œ ì‹¤í–‰

## 9ëŒ€ ê¸°í›„ ë¦¬ìŠ¤í¬

1. Coastal Flood (í•´ì•ˆ í™ìˆ˜)
2. Cold Wave (í•œíŒŒ)
3. Drought (ê°€ë­„)
4. High Temperature (ê³ ì˜¨)
5. Inland Flood (ë‚´ë¥™ í™ìˆ˜)
6. Typhoon (íƒœí’)
7. Urban Flood (ë„ì‹œ í™ìˆ˜)
8. Water Scarcity (ë¬¼ ë¶€ì¡±)
9. Wildfire (ì‚°ë¶ˆ)

## ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/your-org/backend_aiops.git
cd backend_aiops

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì…ë ¥

# uv ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)
pip install uv

# ì˜ì¡´ì„± ì„¤ì¹˜
uv pip install -e .
```

## ì‹¤í–‰

### ë¡œì»¬ ì‹¤í–‰

```bash
python main.py
```

### Docker ì‹¤í–‰

```bash
docker build -t backend-aiops .
docker run -d --env-file .env backend-aiops
```

## ìˆ˜ë™ íŠ¸ë¦¬ê±°

PostgreSQLì—ì„œ NOTIFY ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ íŠ¸ë¦¬ê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```sql
-- P(H) ë°°ì¹˜ ì‹¤í–‰
NOTIFY aiops_trigger, 'probability';

-- Hazard Score ë°°ì¹˜ ì‹¤í–‰
NOTIFY aiops_trigger, 'hazard';
```

## ìŠ¤ì¼€ì¤„

- **P(H) ê³„ì‚°**: ë§¤ë…„ 1ì›” 1ì¼ 02:00 (KST)
- **Hazard Score ê³„ì‚°**: ë§¤ë…„ 1ì›” 1ì¼ 04:00 (KST)

## ì•„í‚¤í…ì²˜

```
backend_aiops (ì´ ì €ì¥ì†Œ)
â”œâ”€â”€ ìŠ¤ì¼€ì¤„ëŸ¬ (APScheduler)
â”œâ”€â”€ NOTIFY ë¦¬ìŠ¤ë„ˆ (PostgreSQL)
â””â”€â”€ ë°°ì¹˜ í”„ë¡œì„¸ì„œ (ë©€í‹°í”„ë¡œì„¸ì‹±)

backend_fastapi (ë³„ë„ ì €ì¥ì†Œ)
â”œâ”€â”€ FastAPI ì„œë²„
â””â”€â”€ AAL ë¶„ì„ API (ì‹¤ì‹œê°„)

ê³µìœ  ë¦¬ì†ŒìŠ¤:
â””â”€â”€ PostgreSQL Database
    â”œâ”€â”€ climate_data (ì…ë ¥)
    â”œâ”€â”€ probability_results (P(H) ì¶œë ¥)
    â””â”€â”€ hazard_results (Hazard ì¶œë ¥)
```

## í™˜ê²½ ë³€ìˆ˜

`.env` íŒŒì¼ ì°¸ì¡°:

- `DATABASE_*`: PostgreSQL ì—°ê²° ì •ë³´
- `PROBABILITY_SCHEDULE_*`: P(H) ìŠ¤ì¼€ì¤„ ì„¤ì •
- `HAZARD_SCHEDULE_*`: Hazard ìŠ¤ì¼€ì¤„ ì„¤ì •
- `PARALLEL_WORKERS`: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜

## ê°œë°œ

```bash
# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
uv pip install -e ".[dev]"

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
pytest --cov=aiops
```
```

## 3. backend_fastapiì—ì„œ íŠ¸ë¦¬ê±° ë³´ë‚´ê¸°

`backend_fastapi` ì €ì¥ì†Œì— ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ AIops ë°°ì¹˜ ì‘ì—…ì„ íŠ¸ë¦¬ê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### backend_fastapi/app/services/aiops_trigger.py

```python
import psycopg2
from typing import Literal
from app.core.config import settings

JobType = Literal['probability', 'hazard']


def trigger_aiops_batch(job_type: JobType) -> bool:
    """AIops ë°°ì¹˜ ì‘ì—… íŠ¸ë¦¬ê±°

    Args:
        job_type: 'probability' ë˜ëŠ” 'hazard'

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        conn = psycopg2.connect(
            host=settings.DATABASE_HOST,
            port=settings.DATABASE_PORT,
            dbname=settings.DATABASE_NAME,
            user=settings.DATABASE_USER,
            password=settings.DATABASE_PASSWORD
        )
        conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)

        cursor = conn.cursor()
        cursor.execute(f"NOTIFY aiops_trigger, '{job_type}';")

        cursor.close()
        conn.close()

        return True
    except Exception as e:
        print(f"Failed to trigger AIops batch: {e}")
        return False
```

### FastAPI ì—”ë“œí¬ì¸íŠ¸ ì˜ˆì‹œ

```python
from fastapi import APIRouter, HTTPException
from app.services.aiops_trigger import trigger_aiops_batch

router = APIRouter(prefix="/admin/aiops", tags=["admin"])


@router.post("/trigger/probability")
async def trigger_probability_batch():
    """P(H) ë°°ì¹˜ ì‘ì—… ìˆ˜ë™ íŠ¸ë¦¬ê±° (ê´€ë¦¬ììš©)"""
    success = trigger_aiops_batch('probability')
    if not success:
        raise HTTPException(status_code=500, detail="Failed to trigger batch job")
    return {"message": "Probability batch job triggered successfully"}


@router.post("/trigger/hazard")
async def trigger_hazard_batch():
    """Hazard Score ë°°ì¹˜ ì‘ì—… ìˆ˜ë™ íŠ¸ë¦¬ê±° (ê´€ë¦¬ììš©)"""
    success = trigger_aiops_batch('hazard')
    if not success:
        raise HTTPException(status_code=500, detail="Failed to trigger batch job")
    return {"message": "Hazard batch job triggered successfully"}
```

## 4. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

AIops ì‹œìŠ¤í…œì´ ì‚¬ìš©í•  í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ:

```sql
-- P(H) ê²°ê³¼ ì €ì¥ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS probability_results (
    id SERIAL PRIMARY KEY,
    latitude DECIMAL(10, 6) NOT NULL,
    longitude DECIMAL(10, 6) NOT NULL,
    risk_type VARCHAR(50) NOT NULL,
    probability JSONB,
    bin_data JSONB,
    calculated_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(latitude, longitude, risk_type)
);

CREATE INDEX idx_probability_coords ON probability_results(latitude, longitude);
CREATE INDEX idx_probability_risk_type ON probability_results(risk_type);

-- Hazard Score ê²°ê³¼ ì €ì¥ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS hazard_results (
    id SERIAL PRIMARY KEY,
    latitude DECIMAL(10, 6) NOT NULL,
    longitude DECIMAL(10, 6) NOT NULL,
    risk_type VARCHAR(50) NOT NULL,
    hazard_score DECIMAL(10, 4),
    hazard_score_100 DECIMAL(10, 4),
    hazard_level VARCHAR(20),
    calculated_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(latitude, longitude, risk_type)
);

CREATE INDEX idx_hazard_coords ON hazard_results(latitude, longitude);
CREATE INDEX idx_hazard_risk_type ON hazard_results(risk_type);

-- ë°°ì¹˜ ì‘ì—… ë¡œê·¸ í…Œì´ë¸” (ì„ íƒì‚¬í•­)
CREATE TABLE IF NOT EXISTS batch_job_logs (
    id SERIAL PRIMARY KEY,
    job_type VARCHAR(50) NOT NULL,
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    status VARCHAR(20),
    total_grids INTEGER,
    processed_grids INTEGER,
    failed_grids INTEGER,
    success_rate DECIMAL(5, 2),
    error_message TEXT
);
```

## 5. Agent íŒŒì¼ ë³µì‚¬

`backend_team/ai_agent/agents/aiops/` í´ë”ì˜ ëª¨ë“  agent íŒŒì¼ë“¤ì„ `backend_aiops/aiops/agents/`ë¡œ ë³µì‚¬í•´ì•¼ í•©ë‹ˆë‹¤:

### ë³µì‚¬í•  íŒŒì¼ ëª©ë¡

**probability_calculate/**
- coastal_flood_probability_agent.py
- cold_wave_probability_agent.py
- drought_probability_agent.py
- high_temperature_probability_agent.py
- inland_flood_probability_agent.py
- typhoon_probability_agent.py
- urban_flood_probability_agent.py
- water_scarcity_probability_agent.py
- wildfire_probability_agent.py

**hazard_calculate/**
- coastal_flood_hscore_agent.py
- cold_wave_hscore_agent.py
- drought_hscore_agent.py
- high_temperature_hscore_agent.py
- inland_flood_hscore_agent.py
- typhoon_hscore_agent.py
- urban_flood_hscore_agent.py
- water_scarcity_hscore_agent.py
- wildfire_hscore_agent.py

## 6. ë°°í¬ ê°€ì´ë“œ

### Docker Compose ì˜ˆì‹œ

```yaml
# docker-compose.yml
version: '3.8'

services:
  aiops:
    image: ghcr.io/your-org/backend_team/aiops:main
    container_name: backend_aiops
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      - postgres
    networks:
      - backend_network

networks:
  backend_network:
    external: true
```

### ë©€í‹° ì»¨í…Œì´ë„ˆ ë°°í¬ êµ¬ì¡°

```
ì„œë²„ í™˜ê²½
â”œâ”€â”€ backend_fastapi (ì»¨í…Œì´ë„ˆ 1)
â”‚   â”œâ”€â”€ FastAPI ì„œë²„
â”‚   â””â”€â”€ í¬íŠ¸: 8000
â”‚
â”œâ”€â”€ backend_aiops (ì»¨í…Œì´ë„ˆ 2)
â”‚   â”œâ”€â”€ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”œâ”€â”€ NOTIFY ë¦¬ìŠ¤ë„ˆ
â”‚   â””â”€â”€ ë°°ì¹˜ í”„ë¡œì„¸ì„œ
â”‚
â””â”€â”€ PostgreSQL (ì»¨í…Œì´ë„ˆ 3)
    â””â”€â”€ í¬íŠ¸: 5432
```

## 7. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

ì‹œìŠ¤í…œ ë¡œê·¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# Docker ë¡œê·¸ í™•ì¸
docker logs backend_aiops -f

# íŠ¹ì • ì‹œê°„ëŒ€ ë¡œê·¸
docker logs backend_aiops --since 1h

# ë°°ì¹˜ ì‘ì—… ë¡œê·¸ (DB)
SELECT * FROM batch_job_logs ORDER BY started_at DESC LIMIT 10;
```

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### NOTIFYê°€ ìˆ˜ì‹ ë˜ì§€ ì•ŠëŠ” ê²½ìš°

1. PostgreSQL ì—°ê²° í™•ì¸
2. LISTEN ì±„ë„ëª… í™•ì¸ (ê¸°ë³¸ê°’: `aiops_trigger`)
3. ë°©í™”ë²½ ì„¤ì • í™•ì¸

### ë°°ì¹˜ ì‘ì—…ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°

1. `batch_job_logs` í…Œì´ë¸”ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
2. Docker ë¡œê·¸ í™•ì¸
3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ê²©ì ì¢Œí‘œ ë°ì´í„° í™•ì¸

### ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°

1. íƒ€ì„ì¡´ ì„¤ì • í™•ì¸
2. APScheduler ë¡œê·¸ í™•ì¸
3. cron í‘œí˜„ì‹ ê²€ì¦

## 9. ë‹¤ìŒ ë‹¨ê³„

1. **ìƒˆ ì €ì¥ì†Œ ìƒì„±**: `backend_aiops` GitHub ì €ì¥ì†Œ ìƒì„±
2. **íŒŒì¼ ë³µì‚¬**: Agent íŒŒì¼ë“¤ì„ ìƒˆ ì €ì¥ì†Œë¡œ ë³µì‚¬
3. **í™˜ê²½ ì„¤ì •**: `.env` íŒŒì¼ ì„¤ì •
4. **ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„**: í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìƒì„±
5. **CI/CD ì„¤ì •**: GitHub Actions secrets ì„¤ì •
6. **ë°°í¬**: Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° ë°°í¬
7. **í…ŒìŠ¤íŠ¸**: NOTIFY íŠ¸ë¦¬ê±°ë¡œ ìˆ˜ë™ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
8. **ëª¨ë‹ˆí„°ë§**: ë¡œê·¸ ë° ê²°ê³¼ í™•ì¸

---

## ì°¸ê³ ì‚¬í•­

- ì´ ì‹œìŠ¤í…œì€ FastAPIë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (APScheduler + PostgreSQL NOTIFYë§Œ ì‚¬ìš©)
- ëª¨ë“  agent ì½”ë“œëŠ” ê¸°ì¡´ `backend_team` ì €ì¥ì†Œì—ì„œ ì¬ì‚¬ìš©
- ë°ì´í„°ë² ì´ìŠ¤ëŠ” `backend_fastapi`ì™€ ê³µìœ 
- ë…ë¦½ì ì¸ ë°°í¬ ë° ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
