"""
SKALA Physical Risk AI System - Í∏∞ÌõÑ Í∑∏Î¶¨Îìú Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨
NetCDF ÌååÏùºÏóêÏÑú ÏõîÎ≥Ñ/Ïó∞Í∞Ñ Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìú

Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§: KMA/extracted/KMA/downloads_kma_ssp_gridraw/*/monthly/*.nc
            KMA/extracted/KMA/downloads_kma_ssp_gridraw/*/yearly/*.nc
ÎåÄÏÉÅ ÌÖåÏù¥Î∏î: location_grid, ta_data, rn_data, ta_yearly_data Îì±
ÏòàÏÉÅ Îç∞Ïù¥ÌÑ∞: ÏïΩ 1,000,000Í∞ú Î†àÏΩîÎìú

ÏµúÏ¢Ö ÏàòÏ†ïÏùº: 2025-12-02
"""

import sys
import gzip
import shutil
import tempfile
from pathlib import Path
from datetime import date
from tqdm import tqdm
import numpy as np

from utils import setup_logging, get_db_connection, get_data_dir, table_exists, get_row_count


def decompress_if_gzip(file_path: Path) -> Path:
    """gzip ÏïïÏ∂ï ÌååÏùºÏù¥Î©¥ ÏïïÏ∂ï Ìï¥Ï†ú"""
    import subprocess

    result = subprocess.run(['file', str(file_path)], capture_output=True, text=True)

    if 'gzip' in result.stdout.lower():
        # gzip ÌååÏùºÏù¥Î©¥ ÏïïÏ∂ï Ìï¥Ï†ú
        decompressed_path = Path(tempfile.mktemp(suffix='.nc'))
        with gzip.open(file_path, 'rb') as f_in:
            with open(decompressed_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        return decompressed_path

    return file_path


def load_climate_grid() -> None:
    """Í∏∞ÌõÑ Í∑∏Î¶¨Îìú Îç∞Ïù¥ÌÑ∞Î•º ÌÖåÏù¥Î∏îÎì§Ïóê Î°úÎìú"""
    logger = setup_logging("load_climate_grid")
    logger.info("=" * 60)
    logger.info("Í∏∞ÌõÑ Í∑∏Î¶¨Îìú Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏãúÏûë")
    logger.info("=" * 60)

    try:
        import netCDF4 as nc
    except ImportError:
        logger.error("‚ùå netCDF4 Î™®ÎìàÏù¥ ÌïÑÏöîÌï©ÎãàÎã§. pip install netCDF4")
        sys.exit(1)

    try:
        conn = get_db_connection()
        logger.info("‚úÖ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ±Í≥µ")
    except Exception as e:
        logger.error(f"‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®: {e}")
        sys.exit(1)

    cursor = conn.cursor()

    # Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨
    data_dir = get_data_dir()
    kma_dir = data_dir / "KMA" / "extracted" / "KMA" / "downloads_kma_ssp_gridraw"

    if not kma_dir.exists():
        logger.error(f"‚ùå KMA ÎîîÎ†âÌÜ†Î¶¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {kma_dir}")
        conn.close()
        sys.exit(1)

    # SSP ÏãúÎÇòÎ¶¨Ïò§ ÎîîÎ†âÌÜ†Î¶¨ Ï∞æÍ∏∞
    ssp_dirs = list(kma_dir.glob("SSP*"))
    logger.info(f"üìÇ {len(ssp_dirs)}Í∞ú SSP ÏãúÎÇòÎ¶¨Ïò§ Î∞úÍ≤¨")

    # 1. location_grid ÌÖåÏù¥Î∏î ÏÉùÏÑ±/Ï¥àÍ∏∞Ìôî
    logger.info("\nüìä location_grid ÌÖåÏù¥Î∏î Ï¥àÍ∏∞Ìôî")
    cursor.execute("TRUNCATE TABLE location_grid CASCADE")
    conn.commit()

    # Ï≤´ Î≤àÏß∏ NetCDFÏóêÏÑú Í∑∏Î¶¨Îìú Ï¢åÌëú Ï∂îÏ∂ú
    sample_files = list(kma_dir.glob("**/monthly/*_ta_*.nc"))
    if not sample_files:
        sample_files = list(kma_dir.glob("**/*.nc"))

    if not sample_files:
        logger.error("‚ùå NetCDF ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
        conn.close()
        sys.exit(1)

    # ÏÉòÌîå ÌååÏùºÏóêÏÑú Í∑∏Î¶¨Îìú Ï¢åÌëú Ï∂îÏ∂ú
    sample_file = decompress_if_gzip(sample_files[0])
    ds = nc.Dataset(sample_file)

    lon_name = 'longitude' if 'longitude' in ds.variables else 'lon'
    lat_name = 'latitude' if 'latitude' in ds.variables else 'lat'

    lon = ds.variables[lon_name][:]
    lat = ds.variables[lat_name][:]

    logger.info(f"   Í∑∏Î¶¨Îìú: {len(lon)} x {len(lat)} = {len(lon) * len(lat):,} points")

    # ÌïúÎ∞òÎèÑ ÏòÅÏó≠Îßå ÌïÑÌÑ∞ÎßÅ (Ïú†Ìö® Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäî ÏòÅÏó≠)
    # Ï≤´ Î≤àÏß∏ Î≥ÄÏàòÏóêÏÑú Ïú†Ìö® Îç∞Ïù¥ÌÑ∞ ÏúÑÏπò Ï∞æÍ∏∞
    var_names = [v for v in ds.variables.keys() if v.upper() not in ['TIME', 'LON', 'LAT', 'LONGITUDE', 'LATITUDE']]
    if var_names:
        sample_data = ds.variables[var_names[0]][0] if len(ds.variables[var_names[0]].shape) > 2 else ds.variables[var_names[0]][:]

        # Ïú†Ìö® Îç∞Ïù¥ÌÑ∞ Ïù∏Îç±Ïä§ Ï∞æÍ∏∞
        valid_mask = ~np.ma.getmaskarray(sample_data)
        valid_lat_idx = np.where(valid_mask.any(axis=1))[0]
        valid_lon_idx = np.where(valid_mask.any(axis=0))[0]

        if len(valid_lat_idx) > 0 and len(valid_lon_idx) > 0:
            lat_start, lat_end = valid_lat_idx[0], valid_lat_idx[-1] + 1
            lon_start, lon_end = valid_lon_idx[0], valid_lon_idx[-1] + 1
        else:
            lat_start, lat_end = 0, len(lat)
            lon_start, lon_end = 0, len(lon)
    else:
        lat_start, lat_end = 0, len(lat)
        lon_start, lon_end = 0, len(lon)

    ds.close()

    # location_gridÏóê Í∑∏Î¶¨Îìú Ìè¨Ïù∏Ìä∏ ÏÇΩÏûÖ
    grid_map = {}  # (lon_idx, lat_idx) -> grid_id
    insert_count = 0

    logger.info(f"   Ïú†Ìö® Î≤îÏúÑ: lat[{lat_start}:{lat_end}], lon[{lon_start}:{lon_end}]")

    for lat_idx in tqdm(range(lat_start, lat_end, 5), desc="Í∑∏Î¶¨Îìú ÏÉùÏÑ±"):  # 5 Í∞ÑÍ≤©ÏúºÎ°ú ÏÉòÌîåÎßÅ
        for lon_idx in range(lon_start, lon_end, 5):
            cursor.execute("""
                INSERT INTO location_grid (longitude, latitude, geom)
                VALUES (%s, %s, ST_SetSRID(ST_MakePoint(%s, %s), 4326))
                RETURNING grid_id
            """, (float(lon[lon_idx]), float(lat[lat_idx]), float(lon[lon_idx]), float(lat[lat_idx])))
            grid_id = cursor.fetchone()[0]
            grid_map[(lon_idx, lat_idx)] = grid_id
            insert_count += 1

    conn.commit()
    logger.info(f"   ‚úÖ location_grid: {insert_count:,}Í∞ú Ìè¨Ïù∏Ìä∏")

    # 2. ÏõîÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Î°úÎìú (ta_data, rn_data)
    logger.info("\nüìä ÏõîÎ≥Ñ Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞ Î°úÎî©")

    # ÌÖåÏù¥Î∏îÎ≥Ñ Î≥ÄÏàò Îß§Ìïë
    table_var_map = {
        'ta_data': ['ta', 'TA'],       # Í∏∞Ïò®
        'rn_data': ['rn', 'RN'],       # Í∞ïÏàòÎüâ
        'rhm_data': ['rhm', 'RHM'],    # ÏÉÅÎåÄÏäµÎèÑ
        'ws_data': ['ws', 'WS'],       # ÌíçÏÜç
    }

    for ssp_dir in ssp_dirs:
        ssp_name = ssp_dir.name  # SSP126, SSP245 Îì±
        monthly_dir = ssp_dir / "monthly"

        if not monthly_dir.exists():
            continue

        logger.info(f"\n   üìÇ {ssp_name} Ï≤òÎ¶¨ Ï§ë...")

        for table_name, var_names in table_var_map.items():
            if not table_exists(conn, table_name):
                logger.warning(f"   ‚ö†Ô∏è  {table_name} ÌÖåÏù¥Î∏î ÏóÜÏùå, Í±¥ÎÑàÎúÄ")
                continue

            # Ìï¥Îãπ Î≥ÄÏàò ÌååÏùº Ï∞æÍ∏∞
            nc_files = []
            for var in var_names:
                nc_files.extend(list(monthly_dir.glob(f"*_{var}_*.nc")))
                nc_files.extend(list(monthly_dir.glob(f"*_{var.lower()}_*.nc")))

            if not nc_files:
                continue

            nc_file = decompress_if_gzip(nc_files[0])

            try:
                ds = nc.Dataset(nc_file)

                # Î≥ÄÏàò Ï∞æÍ∏∞
                data_var = None
                for v in ds.variables.keys():
                    if v.upper() in [vn.upper() for vn in var_names]:
                        data_var = v
                        break

                if not data_var:
                    ds.close()
                    continue

                data = ds.variables[data_var][:]
                time_steps = data.shape[0]

                # SSP Ïª¨Îüº Îß§Ìïë
                ssp_col = {
                    'SSP126': 'ssp1',
                    'SSP245': 'ssp2',
                    'SSP370': 'ssp3',
                    'SSP585': 'ssp5',
                }.get(ssp_name, 'ssp1')

                # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Îäî Ï≤´ Î≤àÏß∏ SSPÏóêÏÑúÎßå ÏÇ≠Ï†ú
                if ssp_name == 'SSP126':
                    cursor.execute(f"TRUNCATE TABLE {table_name}")
                    conn.commit()

                insert_count = 0
                for t_idx in tqdm(range(min(time_steps, 960)), desc=f"  {table_name}", leave=False):  # 80ÎÖÑ * 12Í∞úÏõî
                    year = 2021 + (t_idx // 12)
                    month = (t_idx % 12) + 1
                    obs_date = date(year, month, 1)

                    for (lon_idx, lat_idx), grid_id in grid_map.items():
                        if lat_idx >= data.shape[1] or lon_idx >= data.shape[2]:
                            continue

                        val = data[t_idx, lat_idx, lon_idx]
                        if np.ma.is_masked(val) or np.isnan(val):
                            continue

                        if ssp_name == 'SSP126':
                            cursor.execute(f"""
                                INSERT INTO {table_name} (grid_id, observation_date, {ssp_col})
                                VALUES (%s, %s, %s)
                            """, (grid_id, obs_date, float(val)))
                        else:
                            cursor.execute(f"""
                                UPDATE {table_name}
                                SET {ssp_col} = %s
                                WHERE grid_id = %s AND observation_date = %s
                            """, (float(val), grid_id, obs_date))

                        insert_count += 1

                    if t_idx % 12 == 0:
                        conn.commit()

                conn.commit()
                ds.close()
                logger.info(f"   ‚úÖ {table_name} ({ssp_name}): {insert_count:,}Í∞ú")

            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è  {table_name} Ïò§Î•ò: {e}")

    # 3. Ïó∞Í∞Ñ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    logger.info("\nüìä Ïó∞Í∞Ñ Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞ Î°úÎî©")

    for ssp_dir in ssp_dirs:
        ssp_name = ssp_dir.name
        yearly_dir = ssp_dir / "yearly"

        if not yearly_dir.exists():
            continue

        # ta_yearly_data
        if table_exists(conn, "ta_yearly_data"):
            nc_files = list(yearly_dir.glob("*aii*.nc")) + list(yearly_dir.glob("*ta*.nc"))

            if nc_files:
                nc_file = decompress_if_gzip(nc_files[0])

                try:
                    ds = nc.Dataset(nc_file)

                    # Î≥ÄÏàò Ï∞æÍ∏∞
                    var_names = [v for v in ds.variables.keys()
                                if v.upper() not in ['TIME', 'LON', 'LAT', 'LONGITUDE', 'LATITUDE']]

                    if var_names:
                        data = ds.variables[var_names[0]][:]

                        ssp_col = {
                            'SSP126': 'ssp1', 'SSP245': 'ssp2',
                            'SSP370': 'ssp3', 'SSP585': 'ssp5'
                        }.get(ssp_name, 'ssp1')

                        if ssp_name == 'SSP126':
                            cursor.execute("TRUNCATE TABLE ta_yearly_data")
                            conn.commit()

                        insert_count = 0
                        for year_idx in range(min(data.shape[0], 80)):
                            year = 2021 + year_idx

                            for (lon_idx, lat_idx), grid_id in grid_map.items():
                                if lat_idx >= data.shape[1] or lon_idx >= data.shape[2]:
                                    continue

                                val = data[year_idx, lat_idx, lon_idx]
                                if np.ma.is_masked(val) or np.isnan(val):
                                    continue

                                if ssp_name == 'SSP126':
                                    cursor.execute(f"""
                                        INSERT INTO ta_yearly_data (grid_id, year, {ssp_col})
                                        VALUES (%s, %s, %s)
                                    """, (grid_id, year, float(val)))
                                else:
                                    cursor.execute(f"""
                                        UPDATE ta_yearly_data
                                        SET {ssp_col} = %s
                                        WHERE grid_id = %s AND year = %s
                                    """, (float(val), grid_id, year))

                                insert_count += 1
                                break  # Ï≤´ Î≤àÏß∏ Ïú†Ìö®Í∞íÎßå

                        conn.commit()
                        ds.close()
                        logger.info(f"   ‚úÖ ta_yearly_data ({ssp_name}): {insert_count:,}Í∞ú")

                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è  ta_yearly_data Ïò§Î•ò: {e}")

    # Í≤∞Í≥º ÏöîÏïΩ
    logger.info("\n" + "=" * 60)
    logger.info("‚úÖ Í∏∞ÌõÑ Í∑∏Î¶¨Îìú Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏôÑÎ£å")
    logger.info(f"   - location_grid: {get_row_count(conn, 'location_grid'):,}Í∞ú")
    logger.info(f"   - ta_data: {get_row_count(conn, 'ta_data'):,}Í∞ú")
    logger.info(f"   - rn_data: {get_row_count(conn, 'rn_data'):,}Í∞ú")
    logger.info(f"   - ta_yearly_data: {get_row_count(conn, 'ta_yearly_data'):,}Í∞ú")
    logger.info("=" * 60)

    cursor.close()
    conn.close()


if __name__ == "__main__":
    load_climate_grid()
