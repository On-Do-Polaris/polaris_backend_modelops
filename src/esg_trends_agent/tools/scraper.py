"""
==================================================================
[ëª¨ë“ˆëª…] tools/scraper.py
ESG Economy ì›¹ ìŠ¤í¬ë˜í•‘

[ëª¨ë“ˆ ëª©í‘œ]
1) esgeconomy.com ë‰´ìŠ¤ ìˆ˜ì§‘
2) HTML íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ
==================================================================
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date
from typing import Dict, List, Optional
from ..utils.config import Config
from ..utils.logging import get_logger

logger = get_logger("esg_agent.scraper")

# ESG Economy ë‰´ìŠ¤ URL
ESG_ECONOMY_NEWS_URL = "https://www.esgeconomy.com/news/articleList.html?view_type=sm"


def scrape_esg_economy(limit: int = 10, target_date: Optional[date] = None, max_retries: int = 3) -> List[Dict]:
    """ESG Economy ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘

    Args:
        limit: ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜
        target_date: íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘ (ì—†ìœ¼ë©´ ì „ì²´)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

    Returns:
        List[Dict]: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    import time

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    params = {
        "sc_section_code": "",
        "sc_sub_section_code": "",
        "sc_serial_code": "",
        "sc_area": "",
        "sc_level": "",
        "sc_article_type": "",
        "sc_view_level": "",
        "sc_sdate": "",
        "sc_edate": "",
        "sc_serial_number": "",
        "sc_word": "",
        "sc_order_by": "E",  # ìµœì‹ ìˆœ
        "view_type": "sm",
        "page": 1,
    }

    for attempt in range(max_retries):
        try:
            response = requests.get(ESG_ECONOMY_NEWS_URL, params=params, headers=headers, timeout=15)

            # 429/500/502/503 ì—ëŸ¬ ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
            if response.status_code in [429, 500, 502, 503]:
                wait_time = (attempt + 1) * 5
                logger.warning(f"ESG Economy {response.status_code} ì—ëŸ¬, {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
                continue

            response.raise_for_status()

            soup = BeautifulSoup(response.text, "lxml")
            news_list = parse_esg_economy_page(soup, limit, target_date)

            logger.info(f"ESG Economy ë‰´ìŠ¤ {len(news_list)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

            # ê²°ê³¼ ì¶œë ¥
            if news_list:
                print(f"\n{'='*50}")
                print(f"ğŸ‡°ğŸ‡· ESG Economy êµ­ë‚´ ë‰´ìŠ¤ ({len(news_list)}ê±´)")
                print(f"{'='*50}")
                for i, news in enumerate(news_list[:5], 1):
                    print(f"\n[{i}] [{news.get('category', '')}] {news.get('title', '')[:40]}...")
                    print(f"    ìš”ì•½: {news.get('summary', '')[:50]}...")
                    print(f"    URL: {news.get('url', '')[:50]}...")

            return news_list

        except requests.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 3
                logger.warning(f"ESG Economy ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨, {wait_time}ì´ˆ í›„ ì¬ì‹œë„: {e}")
                time.sleep(wait_time)
            else:
                logger.error(f"ESG Economy ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {e}")
                return []
        except Exception as e:
            logger.error(f"ESG Economy íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    logger.error("ESG Economy ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
    return []


def parse_esg_economy_page(soup: BeautifulSoup, limit: int, target_date: Optional[date]) -> List[Dict]:
    """ESG Economy í˜ì´ì§€ íŒŒì‹±

    Args:
        soup: BeautifulSoup ê°ì²´
        limit: ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜
        target_date: íŠ¹ì • ë‚ ì§œ í•„í„°

    Returns:
        List[Dict]: íŒŒì‹±ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    news_list = []

    # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì°¾ê¸° (li ë˜ëŠ” article íƒœê·¸)
    articles = soup.find_all("li")
    if not articles:
        articles = soup.find_all("article")

    for article in articles:
        if len(news_list) >= limit:
            break

        try:
            # h4.titlesì—ì„œ ì œëª©ê³¼ URL ì¶”ì¶œ
            title_tag = article.find("h4", class_="titles")
            if not title_tag:
                continue

            link = title_tag.find("a")
            if not link:
                continue

            title = link.get_text(strip=True)
            url = link.get("href", "")
            if url and not url.startswith("http"):
                url = f"https://www.esgeconomy.com{url}"

            # p.leadì—ì„œ ìš”ì•½ ì¶”ì¶œ
            summary_tag = article.find("p", class_="lead")
            summary = summary_tag.get_text(strip=True) if summary_tag else ""

            # span.bylineì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            date_tag = article.find("span", class_="byline")
            published_at = ""
            if date_tag:
                date_text = date_tag.get_text(strip=True)
                if "." in date_text:
                    published_at = date_text.split(" ")[0]

            # ESG ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§ (ê´‘ê³ ì„± ê¸°ì‚¬ ì œì™¸)
            esg_keywords = ["ESG", "íƒ„ì†Œ", "í™˜ê²½", "ê¸°í›„", "ì§€ì†ê°€ëŠ¥", "ì¬ìƒì—ë„ˆì§€", "ë…¹ìƒ‰", "CBAM", "ì§€ë°°êµ¬ì¡°"]
            is_esg_related = any(kw in title for kw in esg_keywords)

            # ESG ê´€ë ¨ ê¸°ì‚¬ë§Œ í¬í•¨ (ë˜ëŠ” ê²°ê³¼ê°€ ë¶€ì¡±í•  ê²½ìš° ëª¨ë‘ í¬í•¨)
            if is_esg_related or len(news_list) < 3:
                news = {
                    "title": title,
                    "summary": summary,
                    "url": url,
                    "published_at": published_at,
                    "category": classify_esg_category(title + " " + summary),
                    "source": "ESG Economy",
                }
                news_list.append(news)

        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

    return news_list


def parse_article_item(article) -> Optional[Dict]:
    """ê°œë³„ ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹±

    Args:
        article: BeautifulSoup íƒœê·¸ ê°ì²´

    Returns:
        Dict: íŒŒì‹±ëœ ë‰´ìŠ¤ ë°ì´í„° ë˜ëŠ” None
    """
    # ì œëª© ë° URL
    title_tag = article.find("h4", class_="titles")
    if not title_tag:
        return None

    title_link = title_tag.find("a")
    if not title_link:
        return None

    title = title_link.get_text(strip=True)
    url = title_link.get("href", "")
    if url and not url.startswith("http"):
        url = f"https://www.esgeconomy.com{url}"

    # ìš”ì•½
    summary_tag = article.find("p", class_="lead")
    summary = summary_tag.get_text(strip=True) if summary_tag else ""

    # ë‚ ì§œ
    date_tag = article.find("span", class_="byline")
    published_at = ""
    if date_tag:
        date_text = date_tag.get_text(strip=True)
        # "2024.01.15 10:30" í˜•íƒœì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ
        if "." in date_text:
            published_at = date_text.split(" ")[0]

    # ì¹´í…Œê³ ë¦¬ (E, S, G ë¶„ë¥˜)
    category = classify_esg_category(title + " " + summary)

    return {
        "title": title,
        "summary": summary,
        "url": url,
        "published_at": published_at,
        "category": category,
        "source": "ESG Economy",
    }


def classify_esg_category(text: str) -> str:
    """ESG ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜

    Args:
        text: ë‰´ìŠ¤ ì œëª© + ìš”ì•½

    Returns:
        str: E, S, G, ë˜ëŠ” ESG
    """
    text_lower = text.lower()

    e_keywords = ["í™˜ê²½", "íƒ„ì†Œ", "ê¸°í›„", "ì—ë„ˆì§€", "ì¬ìƒ", "ë°°ì¶œ", "ê·¸ë¦°", "ì¹œí™˜ê²½", "í™˜ê²½ë¶€"]
    s_keywords = ["ì‚¬íšŒ", "ì¸ê¶Œ", "ë…¸ë™", "ì•ˆì „", "ë‹¤ì–‘ì„±", "ì§€ì—­ì‚¬íšŒ", "ë³µì§€", "ê³ ìš©"]
    g_keywords = ["ì§€ë°°êµ¬ì¡°", "ì´ì‚¬íšŒ", "íˆ¬ëª…ì„±", "ìœ¤ë¦¬", "ê°ì‚¬", "ê²½ì˜", "ì£¼ì£¼"]

    e_score = sum(1 for k in e_keywords if k in text_lower)
    s_score = sum(1 for k in s_keywords if k in text_lower)
    g_score = sum(1 for k in g_keywords if k in text_lower)

    if e_score > s_score and e_score > g_score:
        return "E"
    elif s_score > e_score and s_score > g_score:
        return "S"
    elif g_score > e_score and g_score > s_score:
        return "G"
    else:
        return "ESG"


def scrape_today_news(limit: int = 10) -> List[Dict]:
    """ì˜¤ëŠ˜ ë‚ ì§œ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘

    Args:
        limit: ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜

    Returns:
        List[Dict]: ì˜¤ëŠ˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    today = date.today()
    news_list = scrape_esg_economy(limit=limit * 2, target_date=today)

    # ì˜¤ëŠ˜ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìµœì‹  ë‰´ìŠ¤ ë°˜í™˜
    if not news_list:
        logger.info("ì˜¤ëŠ˜ ë‰´ìŠ¤ê°€ ì—†ì–´ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘")
        news_list = scrape_esg_economy(limit=limit)

    return news_list[:limit]
